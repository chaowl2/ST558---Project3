[
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Diabetes EDA",
    "section": "",
    "text": "library(tidyverse)\nlibrary(janitor)\nlibrary(scales)\nlibrary(patchwork)"
  },
  {
    "objectID": "eda.html#introduction",
    "href": "eda.html#introduction",
    "title": "Diabetes EDA",
    "section": "Introduction",
    "text": "Introduction\nThis project analyzes the Diabetes Health Indicators (BRFSS 2015) dataset, which contains demographic, behavioral, and medical variables related to chronic disease risk. In this EDA, I focus on variables that may help explain or predict diabetes status, including age category, BMI, physical activity, general health ratings, smoking history, and indicators of high blood pressure or high cholesterol.\nThe purpose of this EDA is to understand the structure of the data, explore important predictors of the Diabetes_binary outcome, and identify relationships and patterns that will guide the modeling process. These insights will guide the modeling process, where the ultimate goal is to build and select an accurate predictive model for classifying whether an individual has diabetes."
  },
  {
    "objectID": "eda.html#data",
    "href": "eda.html#data",
    "title": "Diabetes EDA",
    "section": "Data",
    "text": "Data\n\nRead data + convert to lowercase\n\ndata <- read_csv(\"data/diabetes_binary_health_indicators_BRFSS2015.csv\") |>\nclean_names()\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#head(data)/\ncolSums(is.na(data))\n\n       diabetes_binary                high_bp              high_chol \n                     0                      0                      0 \n            chol_check                    bmi                 smoker \n                     0                      0                      0 \n                stroke heart_diseaseor_attack          phys_activity \n                     0                      0                      0 \n                fruits                veggies    hvy_alcohol_consump \n                     0                      0                      0 \n        any_healthcare          no_docbc_cost               gen_hlth \n                     0                      0                      0 \n             ment_hlth              phys_hlth              diff_walk \n                     0                      0                      0 \n                   sex                    age              education \n                     0                      0                      0 \n                income \n                     0 \n\n\n\n\nConvert some variables to factors and give meaningful labels\n\ndata <- data |>\nmutate(\ndiabetes_binary = factor(diabetes_binary, levels = c(0, 1),\nlabels = c(\"No_Diabetes\", \"Diabetes\")),\nhigh_bp  = factor(high_bp,  levels = c(0, 1), labels = c(\"No\", \"Yes\")),\nhigh_chol = factor(high_chol, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\nchol_check = factor(chol_check, level = c(0,1), labels = c(\"No\", \"Yes\")),\nsmoker = factor(smoker, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\nstroke = factor(stroke, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\nheart_diseaseor_attack = factor(heart_diseaseor_attack, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\nphys_activity = factor(phys_activity, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\nfruits = factor(fruits, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\nveggies = factor(veggies, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\nhvy_alcohol_consump = factor(hvy_alcohol_consump, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\nany_healthcare = factor(any_healthcare, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\nno_docbc_cost = factor(no_docbc_cost, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\ngen_hlth <- factor(\n  gen_hlth,\n  levels = 1:5,\n  labels = c(\"Excellent\", \"Very Good\", \"Good\", \"Fair\", \"Poor\")\n),\ndiff_walk = factor(diff_walk, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\nsex = factor(sex, levels = c(0, 1), labels = c(\"Female\", \"Male\")),\neducation <- factor(\n  education,\n  levels = 1:6,\n  labels = c(\n    \"K-8\",\n    \"Grades 1-8\",\n    \"Grades 9-11\",\n    \"High School / GED\",\n    \"Some College\",\n    \"College Graduate\"\n  )\n),\nincome <- factor(\n  income,\n  levels = 1:8,\n  labels = c(\n    \"<10k\",\n    \"10-15k\",\n    \"15-20k\",\n    \"20-25k\",\n    \"25-35k\",\n    \"35-50k\",\n    \"50-75k\",\n    \"75k+\"\n  )\n),\nage = factor(\n      age,\n      levels = 1:13,\n      labels = c(\n        \"18–24\", \"25–29\", \"30–34\", \"35–39\",\n        \"40–44\", \"45–49\", \"50–54\", \"55–59\",\n        \"60–64\", \"65–69\", \"70–74\", \"75–79\",\n        \"80+\"\n      )\n)\n)\n\n\n\nCheck for Missing Values\n\ndata |>\nsummarise(across(everything(), ~ sum(is.na(.)))) |>\npivot_longer(everything(), names_to = \"variable\", values_to = \"n_missing\") |>\narrange(desc(n_missing))\n\n# A tibble: 23 × 2\n   variable               n_missing\n   <chr>                      <int>\n 1 diabetes_binary                0\n 2 high_bp                        0\n 3 high_chol                      0\n 4 chol_check                     0\n 5 bmi                            0\n 6 smoker                         0\n 7 stroke                         0\n 8 heart_diseaseor_attack         0\n 9 phys_activity                  0\n10 fruits                         0\n# ℹ 13 more rows\n\n\nNull Values do not exist in the data.\n\n\nExploring Data using visualizations\n\ndata %>%\n  select(bmi, ment_hlth, phys_hlth) %>%\n  summary()\n\n      bmi          ment_hlth        phys_hlth     \n Min.   :12.00   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.:24.00   1st Qu.: 0.000   1st Qu.: 0.000  \n Median :27.00   Median : 0.000   Median : 0.000  \n Mean   :28.38   Mean   : 3.185   Mean   : 4.242  \n 3rd Qu.:31.00   3rd Qu.: 2.000   3rd Qu.: 3.000  \n Max.   :98.00   Max.   :30.000   Max.   :30.000  \n\n# Histograms\ndata %>%\n  select(bmi, ment_hlth, phys_hlth) %>%\n  pivot_longer(everything(), names_to = \"variable\", values_to = \"value\") %>%\n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 30) +\n  facet_wrap(~ variable, scales = \"free_x\") +\n  labs(\n    x = NULL,\n    y = \"Count\",\n    title = \"Distributions of numeric health indicators\"\n  )\n\n\n\n\nBMI is right-skewed with most individuals falling between 25 and 35. This suggests obesity may be a relevant predictor for diabetes, which aligns with medical expectations.\nMental health days show a heavy spike at 0 (most people report no poor mental health days) and a smaller spike at 30. This creates a highly skewed distribution, but this variable may still capture stress or chronic mental health conditions that relate to diabetes risk.\nPhysical health days follow the same pattern as mental health days: most respondents report 0 physically unhealthy days, with another cluster at 30 days. This variable is extremely right-skewed but may help uncover chronic illness patterns contributing to diabetes.\n\ndata %>%\n  group_by(diabetes_binary) %>%\n  summarise(\n    mean_bmi = mean(bmi, na.rm = TRUE),\n    median_bmi = median(bmi, na.rm = TRUE),\n    sd_bmi = sd(bmi, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n# A tibble: 2 × 4\n  diabetes_binary mean_bmi median_bmi sd_bmi\n  <fct>              <dbl>      <dbl>  <dbl>\n1 No_Diabetes         27.8         27   6.29\n2 Diabetes            31.9         31   7.36\n\nggplot(data, aes(x = diabetes_binary, y = bmi)) +\n  geom_boxplot() +\n  labs(\n    x = \"Diabetes status\",\n    y = \"BMI\",\n    title = \"BMI distribution by diabetes status\"\n  )\n\n\n\n\nIndividuals with diabetes show a higher median BMI and more high-BMI outliers compared to those without diabetes. This indicates that higher BMI is associated with diabetes and will likely be an important predictor in the modeling stage.\n\n\nCategorical predictors vs Diabetes\n\ndata %>%\n  count(gen_hlth, diabetes_binary) %>%\n  group_by(gen_hlth) %>%\n  mutate(prop = n / sum(n)) %>%\n  ggplot(aes(x = gen_hlth, y = prop, fill = diabetes_binary)) +\n  geom_col(position = \"fill\") +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = \"General health\",\n    y = \"Proportion within health category\",\n    fill = \"Diabetes status\",\n    title = \"Proportion with diabetes across general health levels\"\n  )\n\n\n\n\nDiabetes becomes increasingly common as general health worsens. Individuals reporting “Fair” or “Poor” health show a much higher proportion of diabetes compared to those in excellent or very good health, suggesting that overall health status is strongly related to diabetes risk.\n\n\nPhysical Activity vs Diabetes\n\ndata %>%\n  count(phys_activity, diabetes_binary) %>%\n  group_by(phys_activity) %>%\n  mutate(prop = n / sum(n)) %>%\n  ggplot(aes(x = phys_activity, y = prop, fill = diabetes_binary)) +\n  geom_col(position = \"fill\") +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = \"Any physical activity in past 30 days?\",\n    y = \"Proportion within activity group\",\n    fill = \"Diabetes status\",\n    title = \"Diabetes prevalence by physical activity\"\n  )\n\n\n\n\nPeople who report no physical activity have a noticeably higher proportion of diabetes compared to those who were physically active in the past 30 days. This indicates that lack of physical activity is associated with a higher likelihood of diabetes.\n\n\nEducation and Income vs Diabetes\n\n# Education\np_edu <- data %>%\n  count(education, diabetes_binary) %>%\n  group_by(education) %>%\n  mutate(prop = n / sum(n)) %>%\n  ggplot(aes(x = education, y = prop, fill = diabetes_binary)) +\n  geom_col(position = \"fill\") +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = \"Education level\",\n    y = \"Proportion within education group\",\n    fill = \"Diabetes status\",\n    title = \"Diabetes prevalence by education\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Income\np_inc <- data %>%\n  count(income, diabetes_binary) %>%\n  group_by(income) %>%\n  mutate(prop = n / sum(n)) %>%\n  ggplot(aes(x = income, y = prop, fill = diabetes_binary)) +\n  geom_col(position = \"fill\") +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = \"Income level\",\n    y = \"Proportion within income group\",\n    fill = \"Diabetes status\",\n    title = \"Diabetes prevalence by income\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\np_edu\n\n\n\np_inc\n\n\n\n\nDiabetes prevalence decreases as education level increases. Individuals with the lowest education levels show the highest proportion of diabetes, while those with college-level education have much lower rates, suggesting education is inversely related to diabetes risk.\nHigher income levels are associated with lower diabetes prevalence. The lowest income groups show the highest proportion of diabetes, and the rate steadily declines as income increases, indicating a clear negative relationship between income and diabetes risk.\n\n\nChecking other Binary Variables\n\nbinary_vars <- c(\n  \"high_bp\", \"high_chol\", \"chol_check\",\n  \"smoker\", \"stroke\", \"heart_diseaseor_attack\",\n  \"fruits\", \"veggies\", \"hvy_alcohol_consump\"\n)\n\n# Counts and proportions\ndata %>%\n  select(all_of(binary_vars)) %>%\n  summarise(across(everything(), ~ mean(. == \"Yes\") * 100)) %>%\n  pivot_longer(everything(),\n               names_to = \"variable\",\n               values_to = \"percent_yes\")\n\n# A tibble: 9 × 2\n  variable               percent_yes\n  <chr>                        <dbl>\n1 high_bp                      42.9 \n2 high_chol                    42.4 \n3 chol_check                   96.3 \n4 smoker                       44.3 \n5 stroke                        4.06\n6 heart_diseaseor_attack        9.42\n7 fruits                       63.4 \n8 veggies                      81.1 \n9 hvy_alcohol_consump           5.62\n\n\nSeveral key health risk factors are common in the dataset: about 43% of respondents report high blood pressure and 42% report high cholesterol, both of which are strongly linked to diabetes and are likely to be important predictors. Nearly 96% of individuals have had their cholesterol checked, indicating widespread screening. Smoking is also prevalent at 44%, which may contribute modestly to diabetes risk.\nStroke (4%) and heart disease or heart attack (9%)—are less common but show strong clinical ties to diabetes and could help distinguish higher-risk individuals. Lifestyle behaviors vary across respondents: 63% regularly eat fruits and 81% eat vegetables, while heavy alcohol use is relatively rare (6%). These indicators collectively provide meaningful information about metabolic health and are likely to contribute to the model’s ability to predict diabetes.\n\n\nPlotting Binary Variables\n\nplot_binary <- function(var) {\n  ggplot(data, aes_string(x = var, fill = \"diabetes_binary\")) +\n    geom_bar(position = \"fill\") +\n    scale_y_continuous(labels = scales::percent) +\n    labs(\n      x = var,\n      y = \"Proportion within group\",\n      fill = \"Diabetes status\",\n      title = paste(\"Diabetes prevalence by\", var)\n    )\n}\np_bp     <- plot_binary(\"high_bp\")\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\np_chol   <- plot_binary(\"high_chol\")\np_check  <- plot_binary(\"chol_check\")\np_smoke  <- plot_binary(\"smoker\")\np_stroke <- plot_binary(\"stroke\")\np_hd     <- plot_binary(\"heart_diseaseor_attack\")\np_fruit  <- plot_binary(\"fruits\")\np_veg    <- plot_binary(\"veggies\")\np_alc    <- plot_binary(\"hvy_alcohol_consump\")\n\n(p_bp | p_chol | p_check) /\n(p_smoke | p_stroke | p_hd) /\n(p_fruit | p_veg | p_alc)\n\n\n\n\nAcross all binary health indicators, individuals with various health risk factors consistently show higher rates of diabetes. Diabetes prevalence is substantially higher among those with high blood pressure, high cholesterol, a history of stroke, or heart disease/heart attack, highlighting their strong clinical connection to diabetes. Smoking shows only a slight increase in diabetes prevalence, while dietary indicators (fruit and vegetable consumption) display small differences between groups. Heavy alcohol consumption appears weakly related as well. Overall, the indicators most strongly associated with diabetes are cardiovascular-related conditions, which will likely play an important role in the predictive modeling stage.\n[Click here for the Modeling Page](Modeling.html)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ST558 Project 3 – Derek Chao",
    "section": "",
    "text": "This website contains all deliverables for Project 3.\nUse the navigation bar to view:\n- EDA\n- Modeling"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Diabetes EDA",
    "section": "Introduction",
    "text": "Introduction\nThis project analyzes the Diabetes Health Indicators (BRFSS 2015) dataset, which contains demographic, behavioral, and medical variables related to chronic disease risk. In this EDA, I focus on variables that may help explain or predict diabetes status, including age category, BMI, physical activity, general health ratings, smoking history, and indicators of high blood pressure or high cholesterol.\nThe purpose of this EDA is to understand the structure of the data, explore important predictors of the Diabetes_binary outcome, and identify relationships and patterns that will guide the modeling process. These insights will guide the modeling process, where the ultimate goal is to build and select an accurate predictive model for classifying whether an individual has diabetes."
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Diabetes EDA",
    "section": "Data",
    "text": "Data\n\nRead data + convert to lowercase\n\ndata <- read_csv(\"data/diabetes_binary_health_indicators_BRFSS2015.csv\") |>\nclean_names()\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#head(data)/\ncolSums(is.na(data))\n\n       diabetes_binary                high_bp              high_chol \n                     0                      0                      0 \n            chol_check                    bmi                 smoker \n                     0                      0                      0 \n                stroke heart_diseaseor_attack          phys_activity \n                     0                      0                      0 \n                fruits                veggies    hvy_alcohol_consump \n                     0                      0                      0 \n        any_healthcare          no_docbc_cost               gen_hlth \n                     0                      0                      0 \n             ment_hlth              phys_hlth              diff_walk \n                     0                      0                      0 \n                   sex                    age              education \n                     0                      0                      0 \n                income \n                     0 \n\n\n\n\nConvert some variables to factors and give meaningful labels\n\ndata <- data |>\nmutate(\ndiabetes_binary = factor(diabetes_binary, levels = c(0, 1),\nlabels = c(\"No_Diabetes\", \"Diabetes\")),\nhigh_bp  = factor(high_bp,  levels = c(0, 1), labels = c(\"No\", \"Yes\")),\nhigh_chol = factor(high_chol, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\nchol_check = factor(chol_check, level = c(0,1), labels = c(\"No\", \"Yes\")),\nsmoker = factor(smoker, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\nstroke = factor(stroke, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\nheart_diseaseor_attack = factor(heart_diseaseor_attack, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\nphys_activity = factor(phys_activity, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\nfruits = factor(fruits, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\nveggies = factor(veggies, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\nhvy_alcohol_consump = factor(hvy_alcohol_consump, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\nany_healthcare = factor(any_healthcare, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\nno_docbc_cost = factor(no_docbc_cost, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\ngen_hlth <- factor(\n  gen_hlth,\n  levels = 1:5,\n  labels = c(\"Excellent\", \"Very Good\", \"Good\", \"Fair\", \"Poor\")\n),\ndiff_walk = factor(diff_walk, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\nsex = factor(sex, levels = c(0, 1), labels = c(\"Female\", \"Male\")),\neducation <- factor(\n  education,\n  levels = 1:6,\n  labels = c(\n    \"K-8\",\n    \"Grades 1-8\",\n    \"Grades 9-11\",\n    \"High School / GED\",\n    \"Some College\",\n    \"College Graduate\"\n  )\n),\nincome <- factor(\n  income,\n  levels = 1:8,\n  labels = c(\n    \"<10k\",\n    \"10-15k\",\n    \"15-20k\",\n    \"20-25k\",\n    \"25-35k\",\n    \"35-50k\",\n    \"50-75k\",\n    \"75k+\"\n  )\n),\nage = factor(\n      age,\n      levels = 1:13,\n      labels = c(\n        \"18–24\", \"25–29\", \"30–34\", \"35–39\",\n        \"40–44\", \"45–49\", \"50–54\", \"55–59\",\n        \"60–64\", \"65–69\", \"70–74\", \"75–79\",\n        \"80+\"\n      )\n)\n)\n\n\n\nCheck for Missing Values\n\ndata |>\nsummarise(across(everything(), ~ sum(is.na(.)))) |>\npivot_longer(everything(), names_to = \"variable\", values_to = \"n_missing\") |>\narrange(desc(n_missing))\n\n# A tibble: 23 × 2\n   variable               n_missing\n   <chr>                      <int>\n 1 diabetes_binary                0\n 2 high_bp                        0\n 3 high_chol                      0\n 4 chol_check                     0\n 5 bmi                            0\n 6 smoker                         0\n 7 stroke                         0\n 8 heart_diseaseor_attack         0\n 9 phys_activity                  0\n10 fruits                         0\n# ℹ 13 more rows\n\n\nNull Values do not exist in the data.\n\n\nExploring Data using visualizations\n\ndata %>%\n  select(bmi, ment_hlth, phys_hlth) %>%\n  summary()\n\n      bmi          ment_hlth        phys_hlth     \n Min.   :12.00   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.:24.00   1st Qu.: 0.000   1st Qu.: 0.000  \n Median :27.00   Median : 0.000   Median : 0.000  \n Mean   :28.38   Mean   : 3.185   Mean   : 4.242  \n 3rd Qu.:31.00   3rd Qu.: 2.000   3rd Qu.: 3.000  \n Max.   :98.00   Max.   :30.000   Max.   :30.000  \n\n# Histograms\ndata %>%\n  select(bmi, ment_hlth, phys_hlth) %>%\n  pivot_longer(everything(), names_to = \"variable\", values_to = \"value\") %>%\n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 30) +\n  facet_wrap(~ variable, scales = \"free_x\") +\n  labs(\n    x = NULL,\n    y = \"Count\",\n    title = \"Distributions of numeric health indicators\"\n  )\n\n\n\n\nBMI is right-skewed with most individuals falling between 25 and 35. This suggests obesity may be a relevant predictor for diabetes, which aligns with medical expectations.\nMental health days show a heavy spike at 0 (most people report no poor mental health days) and a smaller spike at 30. This creates a highly skewed distribution, but this variable may still capture stress or chronic mental health conditions that relate to diabetes risk.\nPhysical health days follow the same pattern as mental health days: most respondents report 0 physically unhealthy days, with another cluster at 30 days. This variable is extremely right-skewed but may help uncover chronic illness patterns contributing to diabetes.\n\ndata %>%\n  group_by(diabetes_binary) %>%\n  summarise(\n    mean_bmi = mean(bmi, na.rm = TRUE),\n    median_bmi = median(bmi, na.rm = TRUE),\n    sd_bmi = sd(bmi, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n# A tibble: 2 × 4\n  diabetes_binary mean_bmi median_bmi sd_bmi\n  <fct>              <dbl>      <dbl>  <dbl>\n1 No_Diabetes         27.8         27   6.29\n2 Diabetes            31.9         31   7.36\n\nggplot(data, aes(x = diabetes_binary, y = bmi)) +\n  geom_boxplot() +\n  labs(\n    x = \"Diabetes status\",\n    y = \"BMI\",\n    title = \"BMI distribution by diabetes status\"\n  )\n\n\n\n\nIndividuals with diabetes show a higher median BMI and more high-BMI outliers compared to those without diabetes. This indicates that higher BMI is associated with diabetes and will likely be an important predictor in the modeling stage.\n\n\nCategorical predictors vs Diabetes\n\ndata %>%\n  count(gen_hlth, diabetes_binary) %>%\n  group_by(gen_hlth) %>%\n  mutate(prop = n / sum(n)) %>%\n  ggplot(aes(x = gen_hlth, y = prop, fill = diabetes_binary)) +\n  geom_col(position = \"fill\") +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = \"General health\",\n    y = \"Proportion within health category\",\n    fill = \"Diabetes status\",\n    title = \"Proportion with diabetes across general health levels\"\n  )\n\n\n\n\nDiabetes becomes increasingly common as general health worsens. Individuals reporting “Fair” or “Poor” health show a much higher proportion of diabetes compared to those in excellent or very good health, suggesting that overall health status is strongly related to diabetes risk.\n\n\nPhysical Activity vs Diabetes\n\ndata %>%\n  count(phys_activity, diabetes_binary) %>%\n  group_by(phys_activity) %>%\n  mutate(prop = n / sum(n)) %>%\n  ggplot(aes(x = phys_activity, y = prop, fill = diabetes_binary)) +\n  geom_col(position = \"fill\") +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = \"Any physical activity in past 30 days?\",\n    y = \"Proportion within activity group\",\n    fill = \"Diabetes status\",\n    title = \"Diabetes prevalence by physical activity\"\n  )\n\n\n\n\nPeople who report no physical activity have a noticeably higher proportion of diabetes compared to those who were physically active in the past 30 days. This indicates that lack of physical activity is associated with a higher likelihood of diabetes.\n\n\nEducation and Income vs Diabetes\n\n# Education\np_edu <- data %>%\n  count(education, diabetes_binary) %>%\n  group_by(education) %>%\n  mutate(prop = n / sum(n)) %>%\n  ggplot(aes(x = education, y = prop, fill = diabetes_binary)) +\n  geom_col(position = \"fill\") +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = \"Education level\",\n    y = \"Proportion within education group\",\n    fill = \"Diabetes status\",\n    title = \"Diabetes prevalence by education\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Income\np_inc <- data %>%\n  count(income, diabetes_binary) %>%\n  group_by(income) %>%\n  mutate(prop = n / sum(n)) %>%\n  ggplot(aes(x = income, y = prop, fill = diabetes_binary)) +\n  geom_col(position = \"fill\") +\n  scale_y_continuous(labels = percent) +\n  labs(\n    x = \"Income level\",\n    y = \"Proportion within income group\",\n    fill = \"Diabetes status\",\n    title = \"Diabetes prevalence by income\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\np_edu\n\n\n\np_inc\n\n\n\n\nDiabetes prevalence decreases as education level increases. Individuals with the lowest education levels show the highest proportion of diabetes, while those with college-level education have much lower rates, suggesting education is inversely related to diabetes risk.\nHigher income levels are associated with lower diabetes prevalence. The lowest income groups show the highest proportion of diabetes, and the rate steadily declines as income increases, indicating a clear negative relationship between income and diabetes risk.\n\n\nChecking other Binary Variables\n\nbinary_vars <- c(\n  \"high_bp\", \"high_chol\", \"chol_check\",\n  \"smoker\", \"stroke\", \"heart_diseaseor_attack\",\n  \"fruits\", \"veggies\", \"hvy_alcohol_consump\"\n)\n\n# Counts and proportions\ndata %>%\n  select(all_of(binary_vars)) %>%\n  summarise(across(everything(), ~ mean(. == \"Yes\") * 100)) %>%\n  pivot_longer(everything(),\n               names_to = \"variable\",\n               values_to = \"percent_yes\")\n\n# A tibble: 9 × 2\n  variable               percent_yes\n  <chr>                        <dbl>\n1 high_bp                      42.9 \n2 high_chol                    42.4 \n3 chol_check                   96.3 \n4 smoker                       44.3 \n5 stroke                        4.06\n6 heart_diseaseor_attack        9.42\n7 fruits                       63.4 \n8 veggies                      81.1 \n9 hvy_alcohol_consump           5.62\n\n\nSeveral key health risk factors are common in the dataset: about 43% of respondents report high blood pressure and 42% report high cholesterol, both of which are strongly linked to diabetes and are likely to be important predictors. Nearly 96% of individuals have had their cholesterol checked, indicating widespread screening. Smoking is also prevalent at 44%, which may contribute modestly to diabetes risk.\nStroke (4%) and heart disease or heart attack (9%)—are less common but show strong clinical ties to diabetes and could help distinguish higher-risk individuals. Lifestyle behaviors vary across respondents: 63% regularly eat fruits and 81% eat vegetables, while heavy alcohol use is relatively rare (6%). These indicators collectively provide meaningful information about metabolic health and are likely to contribute to the model’s ability to predict diabetes.\n\n\nPlotting Binary Variables\n\nplot_binary <- function(var) {\n  ggplot(data, aes_string(x = var, fill = \"diabetes_binary\")) +\n    geom_bar(position = \"fill\") +\n    scale_y_continuous(labels = scales::percent) +\n    labs(\n      x = var,\n      y = \"Proportion within group\",\n      fill = \"Diabetes status\",\n      title = paste(\"Diabetes prevalence by\", var)\n    )\n}\np_bp     <- plot_binary(\"high_bp\")\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\np_chol   <- plot_binary(\"high_chol\")\np_check  <- plot_binary(\"chol_check\")\np_smoke  <- plot_binary(\"smoker\")\np_stroke <- plot_binary(\"stroke\")\np_hd     <- plot_binary(\"heart_diseaseor_attack\")\np_fruit  <- plot_binary(\"fruits\")\np_veg    <- plot_binary(\"veggies\")\np_alc    <- plot_binary(\"hvy_alcohol_consump\")\n\n(p_bp | p_chol | p_check) /\n(p_smoke | p_stroke | p_hd) /\n(p_fruit | p_veg | p_alc)\n\n\n\n\nAcross all binary health indicators, individuals with various health risk factors consistently show higher rates of diabetes. Diabetes prevalence is substantially higher among those with high blood pressure, high cholesterol, a history of stroke, or heart disease/heart attack, highlighting their strong clinical connection to diabetes. Smoking shows only a slight increase in diabetes prevalence, while dietary indicators (fruit and vegetable consumption) display small differences between groups. Heavy alcohol consumption appears weakly related as well. Overall, the indicators most strongly associated with diabetes are cardiovascular-related conditions, which will likely play an important role in the predictive modeling stage."
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\ntidymodels_prefer()\nlibrary(rsample) \nlibrary(doParallel)\nOur goal is to predict diabetes_binary, and to do so we will compare two models: a classification tree and a random forest. Each model will be evaluated using log-loss as the primary performance metric, with 5-fold cross-validation applied on the 70/30 train/test split to assess how well each approach generalizes to unseen data."
  },
  {
    "objectID": "Modeling.html#data-and-split",
    "href": "Modeling.html#data-and-split",
    "title": "Modeling",
    "section": "Data and split",
    "text": "Data and split\n\ndata <- read_csv(\"data/diabetes_binary_health_indicators_BRFSS2015.csv\", show_col_types = FALSE) |>\n  janitor::clean_names() |>\n  mutate(\n    diabetes_binary = factor(diabetes_binary, levels = c(0, 1),\n    labels = c(\"No_Diabetes\", \"Diabetes\")),\n    high_bp  = factor(high_bp,  levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    high_chol = factor(high_chol, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    chol_check = factor(chol_check, level = c(0,1), labels = c(\"No\", \"Yes\")),\n    smoker = factor(smoker, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    stroke = factor(stroke, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    heart_diseaseor_attack = factor(heart_diseaseor_attack, levels = c(0, 1), labels = c(\"No\",     \"Yes\")),\n    phys_activity = factor(phys_activity, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    fruits = factor(fruits, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    veggies = factor(veggies, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    hvy_alcohol_consump = factor(hvy_alcohol_consump, levels = c(0, 1), labels = c(\"No\", \"Yes\"  )),\n    any_healthcare = factor(any_healthcare, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    no_docbc_cost = factor(no_docbc_cost, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    gen_hlth <- factor(\n      gen_hlth,\n      levels = 1:5,\n      labels = c(\"Excellent\", \"Very Good\", \"Good\", \"Fair\", \"Poor\")\n    ),\n    diff_walk = factor(diff_walk, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    sex = factor(sex, levels = c(0, 1), labels = c(\"Female\", \"Male\")),\n    education <- factor(\n      education,\n      levels = 1:6,\n      labels = c(\n        \"K-8\",\n        \"Grades 1-8\",\n        \"Grades 9-11\",\n        \"High School / GED\",\n        \"Some College\",\n        \"College Graduate\"\n      )\n    ),\n    income <- factor(\n      income,\n      levels = 1:8,\n      labels = c(\n        \"<10k\",\n        \"10-15k\",\n        \"15-20k\",\n        \"20-25k\",\n        \"25-35k\",\n        \"35-50k\",\n        \"50-75k\",\n        \"75k+\"\n      )\n    ),\n    age = factor(\n      age,\n      levels = 1:13,\n      labels = c(\n        \"18–24\", \"25–29\", \"30–34\", \"35–39\",\n        \"40–44\", \"45–49\", \"50–54\", \"55–59\",\n        \"60–64\", \"65–69\", \"70–74\", \"75–79\",\n        \"80+\"\n      )\n    )\n  )\n\n\nSplitting the data\n\nset.seed(123)\ndata_split <- initial_split(data, prop = 0.7, strata = diabetes_binary)\ntrain_data <- training(data_split)\nsmall_train <- train_data |>\n  dplyr::sample_frac(0.2)\ntest_data  <- testing(data_split)\n\n\n\nPredictors\nI am choosing to use all predictors.\n\n\nPre-processing\n\ndiabetes_recipe <- recipe(diabetes_binary ~ ., data = train_data) |>\n  step_zv(all_predictors())\n\nSet-up CV and setup Metric = ‘log-loss’\n\nset.seed(123)\nfolds <- vfold_cv(small_train, v = 5, strata = diabetes_binary)\n\nlogloss_metric <- metric_set(mn_log_loss)\n\nClassification Tree Definition\nA classification tree is a predictive modeling method that uses a tree-like structure to classify observations into categories. The algorithm repeatedly splits the data into smaller and more similar groups based on predictor variables. At each split, it chooses the variable and cutoff that best separate the classes—typically using measures like Gini impurity or information gain. These splits continue until the tree reaches a stopping rule, such as a minimum number of observations in a node. The final leaves of the tree represent predicted classes. Classification trees are easy to interpret because they mimic human decision-making: you follow a sequence of “if–then” rules to arrive at a prediction. They can be unstable and prone to overfitting, which is why ensemble methods like random forests are often used to improve performance.\n\ntree_spec <- decision_tree(\n  mode = \"classification\",\n  cost_complexity = tune(),\n  tree_depth = tune(),\n  min_n = tune()\n) |>\n  set_engine(\"rpart\")\n\ntree_grid_small <- grid_regular(\n  cost_complexity(),\n  tree_depth(range = c(3L, 9L)),\n  min_n(range = c(10L, 40L)),\n  levels = 3   \n)\n\ntree_workflow_small <- workflow() |>\n  add_model(tree_spec) |>\n  add_recipe(diabetes_recipe)\n\n# set up parallel backend\ncl <- makeCluster(parallel::detectCores() - 1)\nregisterDoParallel(cl)\n\n# tuning control (this is what goes in `control =`)\nctrl <- control_grid(verbose = TRUE, allow_par = TRUE)\n\nset.seed(123)\ntree_tuned_small <- tune_grid(\n  tree_workflow_small,\n  resamples = folds,\n  grid      = tree_grid_small,\n  metrics   = logloss_metric,\n  control   = ctrl\n)\n\ni Fold1: preprocessor 1/1\n\n\ni Fold1: preprocessor 1/1, model 1/27\n\n\ni Fold1: preprocessor 1/1, model 1/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 2/27\n\n\ni Fold1: preprocessor 1/1, model 2/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 3/27\n\n\ni Fold1: preprocessor 1/1, model 3/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 4/27\n\n\ni Fold1: preprocessor 1/1, model 4/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 5/27\n\n\ni Fold1: preprocessor 1/1, model 5/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 6/27\n\n\ni Fold1: preprocessor 1/1, model 6/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 7/27\n\n\ni Fold1: preprocessor 1/1, model 7/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 8/27\n\n\ni Fold1: preprocessor 1/1, model 8/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 9/27\n\n\ni Fold1: preprocessor 1/1, model 9/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 10/27\n\n\ni Fold1: preprocessor 1/1, model 10/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 11/27\n\n\ni Fold1: preprocessor 1/1, model 11/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 12/27\n\n\ni Fold1: preprocessor 1/1, model 12/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 13/27\n\n\ni Fold1: preprocessor 1/1, model 13/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 14/27\n\n\ni Fold1: preprocessor 1/1, model 14/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 15/27\n\n\ni Fold1: preprocessor 1/1, model 15/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 16/27\n\n\ni Fold1: preprocessor 1/1, model 16/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 17/27\n\n\ni Fold1: preprocessor 1/1, model 17/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 18/27\n\n\ni Fold1: preprocessor 1/1, model 18/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 19/27\n\n\ni Fold1: preprocessor 1/1, model 19/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 20/27\n\n\ni Fold1: preprocessor 1/1, model 20/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 21/27\n\n\ni Fold1: preprocessor 1/1, model 21/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 22/27\n\n\ni Fold1: preprocessor 1/1, model 22/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 23/27\n\n\ni Fold1: preprocessor 1/1, model 23/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 24/27\n\n\ni Fold1: preprocessor 1/1, model 24/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 25/27\n\n\ni Fold1: preprocessor 1/1, model 25/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 26/27\n\n\ni Fold1: preprocessor 1/1, model 26/27 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 27/27\n\n\ni Fold1: preprocessor 1/1, model 27/27 (predictions)\n\n\ni Fold2: preprocessor 1/1\n\n\ni Fold2: preprocessor 1/1, model 1/27\n\n\ni Fold2: preprocessor 1/1, model 1/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 2/27\n\n\ni Fold2: preprocessor 1/1, model 2/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 3/27\n\n\ni Fold2: preprocessor 1/1, model 3/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 4/27\n\n\ni Fold2: preprocessor 1/1, model 4/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 5/27\n\n\ni Fold2: preprocessor 1/1, model 5/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 6/27\n\n\ni Fold2: preprocessor 1/1, model 6/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 7/27\n\n\ni Fold2: preprocessor 1/1, model 7/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 8/27\n\n\ni Fold2: preprocessor 1/1, model 8/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 9/27\n\n\ni Fold2: preprocessor 1/1, model 9/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 10/27\n\n\ni Fold2: preprocessor 1/1, model 10/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 11/27\n\n\ni Fold2: preprocessor 1/1, model 11/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 12/27\n\n\ni Fold2: preprocessor 1/1, model 12/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 13/27\n\n\ni Fold2: preprocessor 1/1, model 13/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 14/27\n\n\ni Fold2: preprocessor 1/1, model 14/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 15/27\n\n\ni Fold2: preprocessor 1/1, model 15/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 16/27\n\n\ni Fold2: preprocessor 1/1, model 16/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 17/27\n\n\ni Fold2: preprocessor 1/1, model 17/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 18/27\n\n\ni Fold2: preprocessor 1/1, model 18/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 19/27\n\n\ni Fold2: preprocessor 1/1, model 19/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 20/27\n\n\ni Fold2: preprocessor 1/1, model 20/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 21/27\n\n\ni Fold2: preprocessor 1/1, model 21/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 22/27\n\n\ni Fold2: preprocessor 1/1, model 22/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 23/27\n\n\ni Fold2: preprocessor 1/1, model 23/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 24/27\n\n\ni Fold2: preprocessor 1/1, model 24/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 25/27\n\n\ni Fold2: preprocessor 1/1, model 25/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 26/27\n\n\ni Fold2: preprocessor 1/1, model 26/27 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 27/27\n\n\ni Fold2: preprocessor 1/1, model 27/27 (predictions)\n\n\ni Fold3: preprocessor 1/1\n\n\ni Fold3: preprocessor 1/1, model 1/27\n\n\ni Fold3: preprocessor 1/1, model 1/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 2/27\n\n\ni Fold3: preprocessor 1/1, model 2/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 3/27\n\n\ni Fold3: preprocessor 1/1, model 3/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 4/27\n\n\ni Fold3: preprocessor 1/1, model 4/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 5/27\n\n\ni Fold3: preprocessor 1/1, model 5/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 6/27\n\n\ni Fold3: preprocessor 1/1, model 6/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 7/27\n\n\ni Fold3: preprocessor 1/1, model 7/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 8/27\n\n\ni Fold3: preprocessor 1/1, model 8/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 9/27\n\n\ni Fold3: preprocessor 1/1, model 9/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 10/27\n\n\ni Fold3: preprocessor 1/1, model 10/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 11/27\n\n\ni Fold3: preprocessor 1/1, model 11/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 12/27\n\n\ni Fold3: preprocessor 1/1, model 12/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 13/27\n\n\ni Fold3: preprocessor 1/1, model 13/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 14/27\n\n\ni Fold3: preprocessor 1/1, model 14/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 15/27\n\n\ni Fold3: preprocessor 1/1, model 15/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 16/27\n\n\ni Fold3: preprocessor 1/1, model 16/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 17/27\n\n\ni Fold3: preprocessor 1/1, model 17/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 18/27\n\n\ni Fold3: preprocessor 1/1, model 18/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 19/27\n\n\ni Fold3: preprocessor 1/1, model 19/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 20/27\n\n\ni Fold3: preprocessor 1/1, model 20/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 21/27\n\n\ni Fold3: preprocessor 1/1, model 21/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 22/27\n\n\ni Fold3: preprocessor 1/1, model 22/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 23/27\n\n\ni Fold3: preprocessor 1/1, model 23/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 24/27\n\n\ni Fold3: preprocessor 1/1, model 24/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 25/27\n\n\ni Fold3: preprocessor 1/1, model 25/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 26/27\n\n\ni Fold3: preprocessor 1/1, model 26/27 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 27/27\n\n\ni Fold3: preprocessor 1/1, model 27/27 (predictions)\n\n\ni Fold4: preprocessor 1/1\n\n\ni Fold4: preprocessor 1/1, model 1/27\n\n\ni Fold4: preprocessor 1/1, model 1/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 2/27\n\n\ni Fold4: preprocessor 1/1, model 2/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 3/27\n\n\ni Fold4: preprocessor 1/1, model 3/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 4/27\n\n\ni Fold4: preprocessor 1/1, model 4/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 5/27\n\n\ni Fold4: preprocessor 1/1, model 5/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 6/27\n\n\ni Fold4: preprocessor 1/1, model 6/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 7/27\n\n\ni Fold4: preprocessor 1/1, model 7/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 8/27\n\n\ni Fold4: preprocessor 1/1, model 8/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 9/27\n\n\ni Fold4: preprocessor 1/1, model 9/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 10/27\n\n\ni Fold4: preprocessor 1/1, model 10/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 11/27\n\n\ni Fold4: preprocessor 1/1, model 11/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 12/27\n\n\ni Fold4: preprocessor 1/1, model 12/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 13/27\n\n\ni Fold4: preprocessor 1/1, model 13/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 14/27\n\n\ni Fold4: preprocessor 1/1, model 14/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 15/27\n\n\ni Fold4: preprocessor 1/1, model 15/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 16/27\n\n\ni Fold4: preprocessor 1/1, model 16/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 17/27\n\n\ni Fold4: preprocessor 1/1, model 17/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 18/27\n\n\ni Fold4: preprocessor 1/1, model 18/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 19/27\n\n\ni Fold4: preprocessor 1/1, model 19/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 20/27\n\n\ni Fold4: preprocessor 1/1, model 20/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 21/27\n\n\ni Fold4: preprocessor 1/1, model 21/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 22/27\n\n\ni Fold4: preprocessor 1/1, model 22/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 23/27\n\n\ni Fold4: preprocessor 1/1, model 23/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 24/27\n\n\ni Fold4: preprocessor 1/1, model 24/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 25/27\n\n\ni Fold4: preprocessor 1/1, model 25/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 26/27\n\n\ni Fold4: preprocessor 1/1, model 26/27 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 27/27\n\n\ni Fold4: preprocessor 1/1, model 27/27 (predictions)\n\n\ni Fold5: preprocessor 1/1\n\n\ni Fold5: preprocessor 1/1, model 1/27\n\n\ni Fold5: preprocessor 1/1, model 1/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 2/27\n\n\ni Fold5: preprocessor 1/1, model 2/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 3/27\n\n\ni Fold5: preprocessor 1/1, model 3/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 4/27\n\n\ni Fold5: preprocessor 1/1, model 4/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 5/27\n\n\ni Fold5: preprocessor 1/1, model 5/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 6/27\n\n\ni Fold5: preprocessor 1/1, model 6/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 7/27\n\n\ni Fold5: preprocessor 1/1, model 7/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 8/27\n\n\ni Fold5: preprocessor 1/1, model 8/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 9/27\n\n\ni Fold5: preprocessor 1/1, model 9/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 10/27\n\n\ni Fold5: preprocessor 1/1, model 10/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 11/27\n\n\ni Fold5: preprocessor 1/1, model 11/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 12/27\n\n\ni Fold5: preprocessor 1/1, model 12/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 13/27\n\n\ni Fold5: preprocessor 1/1, model 13/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 14/27\n\n\ni Fold5: preprocessor 1/1, model 14/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 15/27\n\n\ni Fold5: preprocessor 1/1, model 15/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 16/27\n\n\ni Fold5: preprocessor 1/1, model 16/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 17/27\n\n\ni Fold5: preprocessor 1/1, model 17/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 18/27\n\n\ni Fold5: preprocessor 1/1, model 18/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 19/27\n\n\ni Fold5: preprocessor 1/1, model 19/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 20/27\n\n\ni Fold5: preprocessor 1/1, model 20/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 21/27\n\n\ni Fold5: preprocessor 1/1, model 21/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 22/27\n\n\ni Fold5: preprocessor 1/1, model 22/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 23/27\n\n\ni Fold5: preprocessor 1/1, model 23/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 24/27\n\n\ni Fold5: preprocessor 1/1, model 24/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 25/27\n\n\ni Fold5: preprocessor 1/1, model 25/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 26/27\n\n\ni Fold5: preprocessor 1/1, model 26/27 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 27/27\n\n\ni Fold5: preprocessor 1/1, model 27/27 (predictions)\n\nstopCluster(cl)\n\ncollect_metrics(tree_tuned_small)\n\n# A tibble: 27 × 9\n   cost_complexity tree_depth min_n .metric     .estimator  mean     n   std_err\n             <dbl>      <int> <int> <chr>       <chr>      <dbl> <int>     <dbl>\n 1    0.0000000001          3    10 mn_log_loss binary     0.406     5 0.0000483\n 2    0.0000000001          3    25 mn_log_loss binary     0.406     5 0.0000483\n 3    0.0000000001          3    40 mn_log_loss binary     0.406     5 0.0000483\n 4    0.0000000001          6    10 mn_log_loss binary     0.353     5 0.00480  \n 5    0.0000000001          6    25 mn_log_loss binary     0.350     5 0.00467  \n 6    0.0000000001          6    40 mn_log_loss binary     0.351     5 0.00416  \n 7    0.0000000001          9    10 mn_log_loss binary     0.387     5 0.00708  \n 8    0.0000000001          9    25 mn_log_loss binary     0.343     5 0.00496  \n 9    0.0000000001          9    40 mn_log_loss binary     0.339     5 0.00546  \n10    0.00000316            3    10 mn_log_loss binary     0.406     5 0.0000483\n# ℹ 17 more rows\n# ℹ 1 more variable: .config <chr>\n\n\n\n\nPick the best tree by mean log loss\n\nbest_tree <- select_best(tree_tuned_small, metric = \"mn_log_loss\")\n\nbest_tree\n\n# A tibble: 1 × 4\n  cost_complexity tree_depth min_n .config         \n            <dbl>      <int> <int> <chr>           \n1    0.0000000001          9    40 pre0_mod09_post0\n\ntree_final_wf <- finalize_workflow(tree_workflow_small, best_tree)\n\ntree_final_fit <- fit(tree_final_wf, data = train_data)\n\n\n\nEvaluate on test set\n\ntree_test_preds <- predict(tree_final_fit, test_data, type = \"prob\") |>\n  bind_cols(test_data |> select(diabetes_binary))\n\nmn_log_loss(tree_test_preds,\n            truth = diabetes_binary,\n            .pred_Diabetes,\n            event_level = \"second\")\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 mn_log_loss binary         0.327\n\ntree_class <- predict(tree_final_fit, test_data, type = \"class\") |>\n  bind_cols(test_data |> select(diabetes_binary))\n\nconf_mat(tree_class, truth = diabetes_binary, estimate = .pred_class)\n\n             Truth\nPrediction    No_Diabetes Diabetes\n  No_Diabetes       64001     8861\n  Diabetes           1500     1743\n\naccuracy(tree_class, truth = diabetes_binary, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.864\n\n\n\n\nConclusion\nTo model the probability of diabetes, I tuned a classification tree using a stratified subsample of the training data. The tuning grid explored a range of values for the tree’s cost-complexity penalty, maximum depth, and minimum node size. After identifying the hyperparameter combination that minimized cross-validated log-loss, I refit the finalized tree on the full 70% training split.\nTo evaluate generalization performance, I predicted class probabilities for the held-out 30% test set and computed log-loss using the predicted probability of the “Diabetes” class. The tuned classification tree achieved a test log-loss of 0.32699, indicating that the model produces reasonably calibrated probability estimates for diabetes risk. This performance will serve as a benchmark when comparing against the random forest.\n\n\nRandom Forest Definition\nA random forest is an ensemble learning method that builds a large number of decision trees and combines their predictions to produce a more accurate and stable classifier. Each tree in the forest is trained on a bootstrap sample of the data (sampling with replacement), and at each split, the algorithm considers only a random subset of predictors rather than all available predictors. These two sources of randomness make the individual trees more diverse. When it’s time to predict, the forest aggregates the results—typically by taking a majority vote for classification problems.\nRandom forests improve on a single tree by reducing overfitting and increasing stability. Individual trees are highly variable and sensitive to small changes in the data, but by averaging many decorrelated trees, random forests lower variance and produce more accurate, reliable predictions on new data.\n\nrf_spec <- rand_forest(\n  mode = \"classification\",\n  mtry  = tune(),\n  trees = 500,\n  min_n = tune()\n) |>\n  set_engine(\"ranger\", importance = \"impurity\")\n\nrf_workflow_small <- workflow() |>\n  add_model(rf_spec) |>\n  add_recipe(diabetes_recipe)\n\n# number of predictors (for mtry upper bound)\np <- length(setdiff(names(small_train), \"diabetes_binary\"))\n\nrf_grid_small <- grid_regular(\n  mtry(range = c(2L, p)),\n  min_n(range = c(5L, 50L)),\n  levels = 3   )\n\ncl <- makeCluster(parallel::detectCores() - 1)\nregisterDoParallel(cl)\n\nctrl <- control_grid(verbose = TRUE, allow_par = TRUE)\n\nset.seed(123)\nrf_tuned_small <- tune_grid(\n  rf_workflow_small,\n  resamples = folds,\n  grid      = rf_grid_small,\n  metrics   = logloss_metric,\n  control   = ctrl\n)\n\ni Fold1: preprocessor 1/1\n\n\ni Fold1: preprocessor 1/1, model 1/9\n\n\ni Fold1: preprocessor 1/1, model 1/9 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 2/9\n\n\ni Fold1: preprocessor 1/1, model 2/9 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 3/9\n\n\ni Fold1: preprocessor 1/1, model 3/9 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 4/9\n\n\ni Fold1: preprocessor 1/1, model 4/9 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 5/9\n\n\ni Fold1: preprocessor 1/1, model 5/9 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 6/9\n\n\ni Fold1: preprocessor 1/1, model 6/9 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 7/9\n\n\ni Fold1: preprocessor 1/1, model 7/9 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 8/9\n\n\ni Fold1: preprocessor 1/1, model 8/9 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 9/9\n\n\ni Fold1: preprocessor 1/1, model 9/9 (predictions)\n\n\ni Fold2: preprocessor 1/1\n\n\ni Fold2: preprocessor 1/1, model 1/9\n\n\ni Fold2: preprocessor 1/1, model 1/9 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 2/9\n\n\ni Fold2: preprocessor 1/1, model 2/9 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 3/9\n\n\ni Fold2: preprocessor 1/1, model 3/9 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 4/9\n\n\ni Fold2: preprocessor 1/1, model 4/9 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 5/9\n\n\ni Fold2: preprocessor 1/1, model 5/9 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 6/9\n\n\ni Fold2: preprocessor 1/1, model 6/9 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 7/9\n\n\ni Fold2: preprocessor 1/1, model 7/9 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 8/9\n\n\ni Fold2: preprocessor 1/1, model 8/9 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 9/9\n\n\ni Fold2: preprocessor 1/1, model 9/9 (predictions)\n\n\ni Fold3: preprocessor 1/1\n\n\ni Fold3: preprocessor 1/1, model 1/9\n\n\ni Fold3: preprocessor 1/1, model 1/9 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 2/9\n\n\ni Fold3: preprocessor 1/1, model 2/9 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 3/9\n\n\ni Fold3: preprocessor 1/1, model 3/9 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 4/9\n\n\ni Fold3: preprocessor 1/1, model 4/9 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 5/9\n\n\ni Fold3: preprocessor 1/1, model 5/9 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 6/9\n\n\ni Fold3: preprocessor 1/1, model 6/9 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 7/9\n\n\ni Fold3: preprocessor 1/1, model 7/9 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 8/9\n\n\ni Fold3: preprocessor 1/1, model 8/9 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 9/9\n\n\ni Fold3: preprocessor 1/1, model 9/9 (predictions)\n\n\ni Fold4: preprocessor 1/1\n\n\ni Fold4: preprocessor 1/1, model 1/9\n\n\ni Fold4: preprocessor 1/1, model 1/9 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 2/9\n\n\ni Fold4: preprocessor 1/1, model 2/9 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 3/9\n\n\ni Fold4: preprocessor 1/1, model 3/9 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 4/9\n\n\ni Fold4: preprocessor 1/1, model 4/9 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 5/9\n\n\ni Fold4: preprocessor 1/1, model 5/9 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 6/9\n\n\ni Fold4: preprocessor 1/1, model 6/9 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 7/9\n\n\ni Fold4: preprocessor 1/1, model 7/9 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 8/9\n\n\ni Fold4: preprocessor 1/1, model 8/9 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 9/9\n\n\ni Fold4: preprocessor 1/1, model 9/9 (predictions)\n\n\ni Fold5: preprocessor 1/1\n\n\ni Fold5: preprocessor 1/1, model 1/9\n\n\ni Fold5: preprocessor 1/1, model 1/9 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 2/9\n\n\ni Fold5: preprocessor 1/1, model 2/9 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 3/9\n\n\ni Fold5: preprocessor 1/1, model 3/9 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 4/9\n\n\ni Fold5: preprocessor 1/1, model 4/9 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 5/9\n\n\ni Fold5: preprocessor 1/1, model 5/9 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 6/9\n\n\ni Fold5: preprocessor 1/1, model 6/9 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 7/9\n\n\ni Fold5: preprocessor 1/1, model 7/9 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 8/9\n\n\ni Fold5: preprocessor 1/1, model 8/9 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 9/9\n\n\ni Fold5: preprocessor 1/1, model 9/9 (predictions)\n\nstopCluster(cl)\n\ncollect_metrics(rf_tuned_small)\n\n# A tibble: 9 × 8\n   mtry min_n .metric     .estimator  mean     n std_err .config        \n  <int> <int> <chr>       <chr>      <dbl> <int>   <dbl> <chr>          \n1     2     5 mn_log_loss binary     0.323     5 0.00154 pre0_mod1_post0\n2     2    27 mn_log_loss binary     0.323     5 0.00157 pre0_mod2_post0\n3     2    50 mn_log_loss binary     0.324     5 0.00150 pre0_mod3_post0\n4    12     5 mn_log_loss binary     0.359     5 0.00664 pre0_mod4_post0\n5    12    27 mn_log_loss binary     0.333     5 0.00368 pre0_mod5_post0\n6    12    50 mn_log_loss binary     0.329     5 0.00370 pre0_mod6_post0\n7    22     5 mn_log_loss binary     0.373     5 0.00779 pre0_mod7_post0\n8    22    27 mn_log_loss binary     0.342     5 0.00577 pre0_mod8_post0\n9    22    50 mn_log_loss binary     0.334     5 0.00398 pre0_mod9_post0\n\n\n\nbest_rf <- select_best(rf_tuned_small, metric = \"mn_log_loss\")\nbest_rf\n\n# A tibble: 1 × 3\n   mtry min_n .config        \n  <int> <int> <chr>          \n1     2    27 pre0_mod2_post0\n\nrf_workflow <- workflow() |>\n  add_model(rf_spec) |>\n  add_recipe(diabetes_recipe)\n\nrf_final_wf <- rf_workflow |> finalize_workflow(best_rf)\n\nrf_final_fit <- fit(rf_final_wf, data = train_data)\n\n\n\nEvaluate on test set\n\nrf_test_preds <- predict(rf_final_fit, test_data, type = \"prob\") |>\n  bind_cols(test_data |> select(diabetes_binary))\n\nmn_log_loss(rf_test_preds,\n            truth = diabetes_binary,\n            .pred_Diabetes,\n            event_level = \"second\")\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 mn_log_loss binary         0.320\n\nrf_class <- predict(rf_final_fit, test_data, type = \"class\") |>\n  bind_cols(test_data |> select(diabetes_binary))\n\nconf_mat(rf_class, truth = diabetes_binary, estimate = .pred_class)\n\n             Truth\nPrediction    No_Diabetes Diabetes\n  No_Diabetes       65175     9983\n  Diabetes            326      621\n\naccuracy(rf_class, truth = diabetes_binary, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.865"
  },
  {
    "objectID": "Modeling.html#final-model-selection",
    "href": "Modeling.html#final-model-selection",
    "title": "Modeling",
    "section": "Final Model Selection",
    "text": "Final Model Selection\nBoth the Classification Tree and Random Forest models were evaluated on the exact same test set. The difference in the number of predicted “Diabetes” cases across the confusion matrices does not reflect differing sample sizes, but rather differences in classification behavior.\nThe Random Forest is more conservative: it predicts far fewer positive cases, pushing more borderline individuals into the majority class (“No_Diabetes”). This reduces false positives but increases false negatives — a common trade-off when working with imbalanced data. Even so, the Random Forest still demonstrates superior overall performance when evaluated using our primary metrics.\nPerformance Metrics\n\n\n\nModel\nLog Loss ↓\nAccuracy ↑\n\n\n\n\nClassification Tree\n0.3269909\n0.8638591\n\n\nRandom Forest\n0.3195887\n0.8645424\n\n\n\nInterpretation\nAccuracy:\n\nThe Random Forest again performs slightly better (0.8645 vs 0.8639), correctly classifying more observations overall.\nWhile the Random Forest predicts fewer individuals as “Diabetes,” it improves the overall fit of the probability model and provides more stable predictions.\nEnsemble methods like random forests reduce variance by averaging many trees, which typically leads to better predictive performance than a single tree.\n\nLog Loss:\n\nLower values indicate better-calibrated probability estimates\nThe Random Forest achieves a lower log loss (0.3196 vs 0.3270), meaning its predicted probabilities more closely match the true outcomes.\n\nConclusion\nDespite being more conservative in its classification decisions, the Random Forest produces better-calibrated probabilities and slightly higher accuracy, making it the overall best-performing model on the test set."
  }
]